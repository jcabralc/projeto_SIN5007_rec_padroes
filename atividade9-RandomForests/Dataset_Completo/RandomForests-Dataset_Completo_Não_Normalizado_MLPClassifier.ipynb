{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade 9 - Classificador Random Forests\n",
    "\n",
    "# Dataset Completo - Não Normalizado\n",
    "\n",
    "# Algoritmo RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "● Semelhante aos anteriores, usando Random Forests\n",
    "\n",
    "● Parâmetros a serem calibrados: <br>\n",
    "    – mtry / max_features (o parâmetro mais sensível): raiz (quadrada) de n, n sendo o número de características (testar outros) <br>\n",
    "    – ntree = 500 (quanto maior melhor, depende do tempo disponível)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "\n",
    "#normalizacao\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Modelagem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Classificador\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Balanceamento das classes\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Validação\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from scipy import stats as st\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random_state=5007\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa dataset\n",
    "\n",
    "df = read_csv('../../data/kag_risk_factors_cervical_cancer.csv')\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substituindo ? por NAN\n",
    "df.replace('?', np.NAN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma as feature em numericas\n",
    "df_processed = df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "# Resultado final com as devidas alteracoes\n",
    "#df_processed.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminação (ou não) de instâncias com missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para features continuas --> preenche com a mediana (para nao ter muito impacto com outliers)\n",
    "\n",
    "Hipotese:\n",
    " - Para features categoricas --> preenche com o valor mais frequente\n",
    " \n",
    "No decorrer dos experimentos validar se a hipotese é aceita ou não.\n",
    "\n",
    "(Uma alternativa seria imputar os valores pelo moda (valor mais frequente), que provavelmente é uma solução ruim, pois a resposta verdadeira pode estar correlacionada com a probabilidade de um valor estar ausente. Pois isso teremos um bias, pois esses valores são privados e a pessoa pode escolher não divulga-los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_feat = ['Age',\n",
    "                   'Number of sexual partners',\n",
    "                   'First sexual intercourse',\n",
    "                   'Num of pregnancies', \n",
    "                   'Smokes (years)',\n",
    "                   'Smokes (packs/year)',\n",
    "                   'Hormonal Contraceptives (years)',\n",
    "                   'IUD (years)',\n",
    "                   'STDs (number)',\n",
    "                   'STDs: Number of diagnosis',\n",
    "                   'STDs: Time since first diagnosis',\n",
    "                   'STDs: Time since last diagnosis'] \n",
    "\n",
    "binary_feat = [  'Smokes',\n",
    "                 'Hormonal Contraceptives',\n",
    "                 'IUD',\n",
    "                 'STDs',\n",
    "                 'STDs:condylomatosis',\n",
    "                 'STDs:cervical condylomatosis',\n",
    "                 'STDs:vaginal condylomatosis',\n",
    "                 'STDs:vulvo-perineal condylomatosis',\n",
    "                 'STDs:syphilis',\n",
    "                 'STDs:pelvic inflammatory disease',\n",
    "                 'STDs:genital herpes',\n",
    "                 'STDs:molluscum contagiosum',\n",
    "                 'STDs:AIDS',\n",
    "                 'STDs:HIV',\n",
    "                 'STDs:Hepatitis B',\n",
    "                 'STDs:HPV',\n",
    "                 'Dx:Cancer',\n",
    "                 'Dx:CIN',\n",
    "                 'Dx:HPV',\n",
    "                 'Dx',\n",
    "                 'Hinselmann',\n",
    "                 'Schiller',\n",
    "                 'Citology']                               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp = df_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preenche com a mediana\n",
    "imp_median = SimpleImputer(missing_values = np.nan, strategy = 'median')\n",
    "df_imp[continuous_feat] = imp_median.fit_transform(df_processed[continuous_feat])\n",
    "\n",
    "# preenche com o valor mais frequente\n",
    "imp_most_freq = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "df_imp[binary_feat] = imp_most_freq.fit_transform(df_processed[binary_feat])\n",
    "\n",
    "#df_imp.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Funcao de Plot dos dados\n",
    "\n",
    "def plot_data(x, y, xlabel, ylabel, pos_label, neg_label, features, axes=None):\n",
    "    plt.rcParams['figure.figsize'] = (10., 7.)\n",
    "    \n",
    "    pos = y[y==1].index\n",
    "    neg = y[y==0].index\n",
    "    \n",
    "    if axes == None:\n",
    "        axes = plt.gca()\n",
    "    \n",
    "    axes.scatter(X.iloc[pos][features[0]], X.iloc[pos][features[1]], marker='o', c='#003f5c', s=50, linewidth=2, label=pos_label)\n",
    "    axes.scatter(X.iloc[neg][features[0]], X.iloc[neg][features[1]], marker='o', c='#ffa600', s=50, linewidth=2, label=neg_label)\n",
    "    \n",
    "#     axes.set_xlim([xmin, xmax])\n",
    "#     axes.set_ylim([ymin, ymax])\n",
    "    \n",
    "    axes.set_xlabel(xlabel, fontsize=12)\n",
    "    axes.set_ylabel(ylabel, fontsize=12)\n",
    "    \n",
    "    axes.legend(bbox_to_anchor=(1,1), fancybox=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = df_imp.drop('Biopsy', axis=1)\n",
    "y = df_imp['Biopsy']\n",
    "\n",
    "# Exmplo\n",
    "features = ['Age', 'First sexual intercourse']\n",
    "\n",
    "plot_data(X, y, features[0], features[1], 'Positivo', 'Negativo', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Cross- Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Desbalanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation com dataset DESBALANCEADO\n",
    "\n",
    "def stratified_k_fold(df, k, algoritmo, random_state, grid_params, shuffle=False, shrinking=True):\n",
    "    print('------')\n",
    "    print('Algorimto Utilizado: {}'.format(algoritmo))\n",
    "    print('------')\n",
    "    \n",
    "    # Utilizando todas as features como preditoras\n",
    "    X = df.drop('Biopsy', axis=1)\n",
    "    y = df['Biopsy']\n",
    "    \n",
    "    # quantidade original de classes\n",
    "    count_classes = y.value_counts()\n",
    "    \n",
    "    # Stratified K Fold\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=random_state)\n",
    "    skf.get_n_splits(X, y)\n",
    "    print('k = {}, Dataset {} positivas e {} negativas ({:.2f}% x {:.2f}%)'.format(k, count_classes[1], \n",
    "                                                                                 count_classes[0], \n",
    "                                                                                 ((count_classes[1]/len(y))*100), \n",
    "                                                                                 ((count_classes[0]/len(y))*100)))\n",
    "        \n",
    "    # Scores (das futuras metricas)\n",
    "    scores = []\n",
    "    \n",
    "    # coleta os parametros que serao testados\n",
    "    max_features = grid_params.get('max_features')\n",
    "    n_estimators = grid_params.get('n_estimators')\n",
    "    #max_depth = grid_params.get('max_depth')\n",
    "    #min_samples_split = grid_params.get('min_samples_split')\n",
    "    #min_samples_leaf = grid_params.get('min_samples_leaf')\n",
    "    #bootstrap = grid_params.get('bootstrap')\n",
    "    \n",
    "    \n",
    "    # Testa varias max_features\n",
    "    for mf in max_features:\n",
    "        print(\"-> Teste max_features {}\".format(mf))\n",
    "        \n",
    "        # Testa n_estimators\n",
    "        for n_estim in n_estimators:\n",
    "            print(\"-> Teste n_estimators {}\".format(mf))\n",
    "            \n",
    "#             # Testa max_depth\n",
    "#             for md in max_depth:\n",
    "#                 print(\"-> Teste max_depth {}\".format(md))\n",
    "                \n",
    "#                 # Testa min_samples_split\n",
    "#                 for min_ss in min_samples_split:\n",
    "#                     print(\"-> Teste min_samples_split {}\".format(min_ss))\n",
    "                    \n",
    "#                     # Testa min_samples_leaf\n",
    "#                     for min_sl in min_samples_leaf:\n",
    "#                         print(\"-> Teste min_samples_leaf {}\".format(min_sl))\n",
    "                        \n",
    "#                         # Testa bootstrap\n",
    "#                         for bs in bootstrap:\n",
    "#                             print('\\t-> Modelo: max_features ={} | n_estimators {} | max_depth {} | min_samples_split {} | min_samples_leaf {} | bootstrap {}'.format(mf, n_estim, md, min_ss, min_sl, bs))\n",
    "#                             clf = algoritmo(max_features=mf, n_estimators=n_estim, max_depth=md, min_samples_split=min_ss, min_samples_leaf=min_sl, bootstrap=bs, verbose=0, random_state = random_state)\n",
    "            print('\\t-> Modelo: max_features ={} | n_estimators {}'.format(mf, n_estim))\n",
    "            clf = algoritmo(max_features=mf, n_estimators=n_estim, verbose=0, random_state = random_state)\n",
    "\n",
    "            # Folds\n",
    "            for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "                fold_number = fold\n",
    "                X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "                X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "\n",
    "                # quantidade de classes dentro da fold\n",
    "                #REMOVED\n",
    "\n",
    "                # aplica o classificador\n",
    "                clf = clf.fit(X_train, y_train)\n",
    "\n",
    "                # predict no dataset de treino \n",
    "                y_train_preds = clf.predict(X_train)\n",
    "                # predict no dataset de teste\n",
    "                y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "                # Scores do model (utilizados dados nao-balanceados) - dados de teste\n",
    "                recall = recall_score(y_test, y_pred)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred)\n",
    "                scores.append([mf, n_estim, fold_number, precision, recall, accuracy]) #md, min_ss, min_sl, bs, fold_number, precision, recall, accuracy])\n",
    "\n",
    "\n",
    "    print(\"Treinamento Finalizado!\")\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation com dataset BALANCEADO (SMOTETomek)\n",
    "\n",
    "def stratified_k_fold_SMOTE(df, k, algoritmo, random_state, grid_params, shuffle=False, shrinking=True):\n",
    "    print('------')\n",
    "    print('Algorimto Utilizado: {}'.format(algoritmo))\n",
    "    print('------')\n",
    "    \n",
    "    # Utilizando todas as features como preditoras\n",
    "    X = df.drop('Biopsy', axis=1)\n",
    "    y = df['Biopsy']\n",
    "    # quantidade original de classes\n",
    "    count_classes = y.value_counts()\n",
    "    # Stratified K Fold\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=random_state)\n",
    "    skf.get_n_splits(X, y)\n",
    "    print('k = {}, Dataset (desbalanceado) {} positivas e {} negativas ({:.2f}% x {:.2f}%)'.format(k, count_classes[1], \n",
    "                                                                                 count_classes[0], \n",
    "                                                                                 ((count_classes[1]/len(y))*100), \n",
    "                                                                                 ((count_classes[0]/len(y))*100)))\n",
    "   \n",
    "    # Normalização MinMax\n",
    "    minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # SMOTETomek\n",
    "    cc = SMOTETomek(random_state=random_state)\n",
    "\n",
    "    # Scores (das futuras metricas)\n",
    "    scores = []\n",
    "    \n",
    "    # coleta os parametros que serao testados\n",
    "    max_features = grid_params.get('max_features')\n",
    "    n_estimators = grid_params.get('n_estimators')\n",
    "    #max_depth = grid_params.get('max_depth')\n",
    "    #min_samples_split = grid_params.get('min_samples_split')\n",
    "    #min_samples_leaf = grid_params.get('min_samples_leaf')\n",
    "    #bootstrap = grid_params.get('bootstrap')\n",
    "    \n",
    "    \n",
    "    # Testa varias max_features\n",
    "    for mf in max_features:\n",
    "        print(\"-> Teste max_features {}\".format(mf))\n",
    "        \n",
    "        # Testa n_estimators\n",
    "        for n_estim in n_estimators:\n",
    "            print(\"-> Teste n_estimators {}\".format(mf))\n",
    "          \n",
    "            # Testa max_depth\n",
    "#             for md in max_depth:\n",
    "#                 print(\"-> Teste max_depth {}\".format(md))\n",
    "\n",
    "#                 # Testa min_samples_split\n",
    "#                 for min_ss in min_samples_split:\n",
    "#                     print(\"-> Teste min_samples_split {}\".format(min_ss))\n",
    "\n",
    "#                     # Testa min_samples_leaf\n",
    "#                     for min_sl in min_samples_leaf:\n",
    "#                         print(\"-> Teste min_samples_leaf {}\".format(min_sl))\n",
    "\n",
    "#                         # Testa bootstrap\n",
    "#                         for bs in bootstrap:\n",
    "#                             print('\\t-> Modelo: max_features ={} | n_estimators {} | max_depth {} | min_samples_split {} | min_samples_leaf {} | bootstrap {}'.format(mf, n_estim, md, min_ss, min_sl, bs))\n",
    "#                             clf = algoritmo(max_features=mf, n_estimators=n_estim, max_depth=md, min_samples_split=min_ss, min_samples_leaf=min_sl, bootstrap=bs,verbose=0, random_state = random_state)\n",
    "            print('\\t-> Modelo: max_features ={} | n_estimators {}'.format(mf, n_estim))\n",
    "            clf = algoritmo(max_features=mf, n_estimators=n_estim, verbose=0, random_state = random_state)\n",
    "\n",
    "\n",
    "            # Folds\n",
    "            for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "                fold_number = fold\n",
    "                X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "                X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "\n",
    "                # Normaliza MinMax para aplicar Smote\n",
    "                X_train_normalized = minmax_scale.fit_transform(X_train)\n",
    "                X_train_normalized = pd.DataFrame(X_train_normalized, columns = X.columns.tolist())\n",
    "\n",
    "                # SMOTETomek (apenas os dados de treino)\n",
    "                # print('\\tBalanceando dados de treino fold {}...'.format(fold_number))\n",
    "                X_train, y_train = cc.fit_resample(X_train_normalized, y_train)\n",
    "\n",
    "                # Retorna para valores não normalizados\n",
    "                X_train = pd.DataFrame(minmax_scale.inverse_transform(X_train), columns = X.columns.tolist())\n",
    "                # print folds balanceados\n",
    "                #REMOVED\n",
    "\n",
    "                # quantidade de classes dentro da fold\n",
    "                #REMOVED\n",
    "\n",
    "                # aplica o classificador\n",
    "                clf = clf.fit(X_train, y_train)\n",
    "\n",
    "                # predict no dataset de treino \n",
    "                y_train_preds = clf.predict(X_train)\n",
    "                # predict no dataset de teste\n",
    "                y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "                # Scores do model (utilizados dados nao-balanceados) - dados de teste\n",
    "                recall = recall_score(y_test, y_pred)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred)\n",
    "                scores.append([mf, n_estim, fold_number, precision, recall, accuracy]) #md, min_ss, min_sl, bs, fold_number, precision, recall, accuracy])\n",
    "\n",
    "    print(\"Treinamento Finalizado!\")\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid de Parametros para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params_all = {'max_features': [\"sqrt\", \"log2\", None], # Number of features to consider at every split\n",
    "                   'n_estimators' : [100] + [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)], # Number of trees in random forest # defautl = 100 \n",
    "                   #'max_depth':  [int(x) for x in np.linspace(10, 110, num = 11)] + [None], # Maximum number of levels in tree\n",
    "                   #'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node\n",
    "                   #'min_samples_leaf': [1, 2, 4], # Minimum number of samples required at each leaf node\n",
    "                   #'bootstrap': [True, False] # Method of selecting samples for training each tree\n",
    "              }\n",
    "\n",
    "pprint(grid_params_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando Precisão, Revocação e Acurácia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algortimo-> RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Desbalanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "print('K-Fold com K = {}'.format(k))\n",
    "scores_unbalanced = pd.DataFrame(stratified_k_fold(df_imp, k, RandomForestClassifier, random_state, grid_params_all, shuffle=False, shrinking=True))\n",
    "# scores_unbalanced = scores_unbalanced.rename(columns = {0: 'max_features', 1: 'n_estimators', 2: 'max_depth', 3: 'min_samples_split', 4: 'min_samples_leaf', 5: 'bootstrap', 6:'Fold', 7: 'Precision', 8: 'Recall', 9: 'Accuracy'}) \n",
    "scores_unbalanced = scores_unbalanced.rename(columns = {0: 'max_features', 1: 'n_estimators', 2:'Fold', 3: 'Precision', 4: 'Recall', 5: 'Accuracy'}) \n",
    "\n",
    "scores_unbalanced['Algorimto'] = 'RandomForestClassifier'\n",
    "\n",
    "#float_cols = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'Fold', 'Precision', 'Recall', 'Accuracy']\n",
    "float_cols = ['n_estimators', 'Fold', 'Precision', 'Recall', 'Accuracy']\n",
    "scores_unbalanced[float_cols] = scores_unbalanced[float_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "#string_cols = ['max_features', 'bootstrap']\n",
    "string_cols = ['max_features']\n",
    "scores_unbalanced[string_cols] = scores_unbalanced[string_cols].astype(str)\n",
    "print('----'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scores_unbalanced.head())\n",
    "\n",
    "#Salva os Scores\n",
    "scores_unbalanced.to_excel('scores_unbalanced-Dataset_Completo_Não_Normalizado_RandomForestClassifier.xlsx', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "print('K-Fold com K = {}'.format(k))\n",
    "scores_balanced = pd.DataFrame(stratified_k_fold_SMOTE(df_imp, k, RandomForestClassifier, random_state, grid_params_all, shuffle=False, shrinking=True))\n",
    "# scores_balanced = scores_balanced.rename(columns = {0: 'max_features', 1: 'n_estimators', 2: 'max_depth', 3: 'min_samples_split', 4: 'min_samples_leaf', 5: 'bootstrap', 6:'Fold', 7: 'Precision', 8: 'Recall', 9: 'Accuracy'}) \n",
    "scores_balanced = scores_balanced.rename(columns = {0: 'max_features', 1: 'n_estimators', 2:'Fold', 3: 'Precision', 4: 'Recall', 5: 'Accuracy'}) \n",
    "\n",
    "scores_balanced['Algorimto'] = 'RandomForestClassifier'\n",
    "\n",
    "#float_cols = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'Fold', 'Precision', 'Recall', 'Accuracy']\n",
    "float_cols = ['n_estimators', 'Fold', 'Precision', 'Recall', 'Accuracy']\n",
    "scores_balanced[float_cols] = scores_balanced[float_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "#string_cols = ['max_features', 'bootstrap']\n",
    "string_cols = ['max_features']\n",
    "scores_balanced[string_cols] = scores_balanced[string_cols].astype(str)\n",
    "print('----'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scores_balanced.head())\n",
    "\n",
    "#Salva os Scores\n",
    "scores_balanced.to_excel('scores_balanced-Dataset_Completo_Não_Normalizado_RandomForestClassifier.xlsx',  encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando os parâmetros com maior acurácia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Desbalanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_unbalanced = scores_unbalanced.groupby(['max_features', 'n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'bootstrap']).mean().drop('Fold', axis = 1)\n",
    "mean_unbalanced = scores_unbalanced.groupby(['max_features', 'n_estimators']).mean().drop('Fold', axis = 1)\n",
    "mean_unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 com maior média\n",
    "display(mean_unbalanced.sort_values(['Accuracy'],ascending=False)[:5])\n",
    "\n",
    "max_alpha_unbalanced = mean_unbalanced['Accuracy'].idxmax()\n",
    "max_alpha_unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_balanced = scores_balanced.groupby(['max_features', 'n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'bootstrap']).mean().drop('Fold', axis = 1)\n",
    "mean_balanced = scores_balanced.groupby(['max_features', 'n_estimators']).mean().drop('Fold', axis = 1)\n",
    "mean_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 com maior média\n",
    "display(mean_balanced.sort_values(['Accuracy'],ascending=False)[:5])\n",
    "\n",
    "max_alpha_balanced = mean_balanced['Accuracy'].idxmax()\n",
    "max_alpha_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando intervalo de confiança da Acurácia para os melhores parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Desbalanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando apenas folds com os melhores params\n",
    "metrics_unbalanced = scores_unbalanced.loc[scores_unbalanced[(scores_unbalanced['max_features'] == max_alpha_unbalanced[0]) &\n",
    "                                                             (scores_unbalanced['n_estimators'] == max_alpha_unbalanced[1])].index, ['Accuracy', 'Recall', 'Precision']]\n",
    "mean_unbalanced = np.mean(metrics_unbalanced)\n",
    "\n",
    "# Calculo do ic\n",
    "acc_min_unbalanced, acc_max_unbalanced = st.t.interval(0.95, len(metrics_unbalanced['Accuracy'])-1, loc=mean_unbalanced[0], scale=st.sem(metrics_unbalanced['Accuracy']))  \n",
    "rec_min_unbalanced, rec_max_unbalanced = st.t.interval(0.95, len(metrics_unbalanced['Recall'])-1, loc=mean_unbalanced[1], scale=st.sem(metrics_unbalanced['Recall']))  \n",
    "pre_min_unbalanced, pre_max_unbalanced = st.t.interval(0.95, len(metrics_unbalanced['Precision'])-1, loc=mean_unbalanced[2], scale=st.sem(metrics_unbalanced['Precision']))  \n",
    "\n",
    "\n",
    "print('Acurácia: Média = {:.3f}, IC = [{:.3f}, {:.3f}]'.format(mean_unbalanced[0],acc_min_unbalanced, acc_max_unbalanced))\n",
    "print('Recall: Média = {:.3f}, IC = [{:.3f}, {:.3f}]'.format(mean_unbalanced[1],rec_min_unbalanced, rec_max_unbalanced))\n",
    "print('Precisão: Média = {:.3f}, IC = [{:.3f}, {:.3f}]'.format(mean_unbalanced[2],pre_min_unbalanced, pre_max_unbalanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando apenas folds com os melhores params\n",
    "metrics_balanced = scores_balanced.loc[scores_balanced[(scores_balanced['max_features'] == max_alpha_balanced[0]) &\n",
    "                                                       (scores_balanced['n_estimators'] == max_alpha_balanced[1])].index, ['Accuracy', 'Recall', 'Precision']]\n",
    "mean_balanced = np.mean(metrics_balanced)\n",
    "\n",
    "# Calculo do ic\n",
    "acc_min_balanced, acc_max_balanced  = st.t.interval(0.95, len(metrics_balanced['Accuracy'])-1, loc=mean_balanced[0], scale=st.sem(metrics_balanced['Accuracy']))  \n",
    "rec_min_balanced, rec_max_balanced = st.t.interval(0.95, len(metrics_balanced['Recall'])-1, loc=mean_balanced[1], scale=st.sem(metrics_balanced['Recall']))  \n",
    "pre_min_balanced, pre_max_balanced = st.t.interval(0.95, len(metrics_balanced['Precision'])-1, loc=mean_balanced[2], scale=st.sem(metrics_balanced['Precision']))  \n",
    "\n",
    "\n",
    "print('Acurácia: Média = {:.3f}, IC = [{:.3f}, {:.3f}]'.format(mean_balanced[0],acc_min_balanced, acc_max_balanced))\n",
    "print('Recall: Média = {:.3f}, IC = [{:.3f}, {:.3f}]'.format(mean_balanced[1],rec_min_balanced, rec_max_balanced))\n",
    "print('Precisão: Média = {:.3f}, IC = [{:.3f}, {:.3f}]'.format(mean_balanced[2],pre_min_balanced, pre_max_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/44542112\n",
    "def change_width(ax, new_value) :\n",
    "    for patch in ax.patches :\n",
    "        current_width = patch.get_width()\n",
    "        diff = current_width - new_value\n",
    "\n",
    "        # we change the bar width\n",
    "        patch.set_width(new_value)\n",
    "\n",
    "        # we recenter the bar\n",
    "        patch.set_x(patch.get_x() + diff * .5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Desbalanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "fig_unbalanced = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Medias dos melhores params\n",
    "df_mean_unbalanced =  pd.DataFrame(mean_unbalanced).rename_axis('Métrica').reset_index().rename(columns = {0: 'Value'}) \n",
    "\n",
    "#plot\n",
    "ax = sns.barplot(x=\"Métrica\", y=\"Value\", data=df_mean_unbalanced, ci = None)\n",
    "plt.title('Dataset completo não balanceado - Nao Normalizado')\n",
    "\n",
    "\n",
    "plt.errorbar(x=[0],y=mean_unbalanced[0],yerr= (acc_max_unbalanced - mean_unbalanced[0]) , fmt='none', color = 'black')\n",
    "plt.errorbar(x=[1],y=mean_unbalanced[1],yerr= (rec_max_unbalanced - mean_unbalanced[1]) , fmt='none', color = 'black')\n",
    "plt.errorbar(x=[2],y=mean_unbalanced[2],yerr= (pre_max_unbalanced - mean_unbalanced[2]) , fmt='none', color = 'black')\n",
    "\n",
    "fig_unbalanced.savefig('Dataset completo nao balanceado - Nao Normalizado - RandomForestClassifier', bbox_inches='tight', dpi=600)\n",
    "\n",
    "ax.set(ylabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "fig_balanced = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Medias dos melhores params\n",
    "df_mean_balanced =  pd.DataFrame(mean_balanced).rename_axis('Métrica').reset_index().rename(columns = {0: 'Value'}) \n",
    "\n",
    "#plot\n",
    "ax = sns.barplot(x=\"Métrica\", y=\"Value\", data=df_mean_balanced, ci = None)\n",
    "plt.title('Dataset completo Balanceado - Nao Normalizado')\n",
    "\n",
    "\n",
    "plt.errorbar(x=[0],y=mean_balanced[0],yerr= (acc_max_balanced - mean_balanced[0]) , fmt='none', color = 'black')\n",
    "plt.errorbar(x=[1],y=mean_balanced[1],yerr= (rec_max_balanced - mean_balanced[1]) , fmt='none', color = 'black')\n",
    "plt.errorbar(x=[2],y=mean_balanced[2],yerr= (pre_max_balanced - mean_balanced[2]) , fmt='none', color = 'black')\n",
    "\n",
    "fig_balanced.savefig('Dataset completo balanceado - Nao Normalizado - RandomForestClassifier', bbox_inches='tight', dpi=600)\n",
    "\n",
    "ax.set(ylabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armazena os Resultados para Gráfico Comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_save = '../../model_results/'\n",
    "score_min_unbalanced = pd.DataFrame([acc_min_unbalanced, rec_min_unbalanced, pre_min_unbalanced]).rename(columns = {0: 'Score'})\n",
    "score_max_unbalanced =  pd.DataFrame([acc_max_unbalanced, rec_max_unbalanced, pre_max_unbalanced]).rename(columns = {0: 'Score'})\n",
    "score_min_balanced = pd.DataFrame([acc_min_balanced, rec_min_balanced, pre_min_balanced]).rename(columns = {0: 'Score'})\n",
    "score_max_balanced =  pd.DataFrame([acc_max_balanced, rec_max_balanced, pre_max_balanced]).rename(columns = {0: 'Score'})\n",
    "\n",
    "# Leitura dos resultados anteriores\n",
    "all_models_results = pd.read_excel(path_to_save+'all_models_results.xlsx')\n",
    "\n",
    "for metrica in df_mean_unbalanced.index:\n",
    "    result = [\n",
    "    'RandomForestClassifier',\n",
    "    '9',\n",
    "    'Dataset Completo Desbalanceado - Nao Normalizado',\n",
    "    max_alpha_unbalanced,\n",
    "    df_mean_unbalanced.loc[metrica]['Métrica'],\n",
    "    df_mean_unbalanced.loc[metrica]['Value'],\n",
    "    '[{:.3f}, {:.3f}, {:.3f}]'.format(mean_unbalanced[metrica],score_min_unbalanced.loc[metrica]['Score'], score_max_unbalanced.loc[metrica]['Score'])\n",
    "    ]\n",
    "    aux_df = (pd.DataFrame(result).T).rename(columns = {0:'Algoritmo' , 1: 'Atividade', 2: 'Condicao',\n",
    "                                                        3: 'Melhores_Params', 4:'Metrica', 5: 'Valor', 6: 'Intervalo_Confianca'})\n",
    "    all_models_results = all_models_results.append(aux_df, ignore_index=True)\n",
    "\n",
    "# deleta da memoria\n",
    "del result, aux_df\n",
    "\n",
    "# Dataset Completo Balanceado\n",
    "for metrica in df_mean_balanced.index:\n",
    "    result = [\n",
    "    'RandomForestClassifier',\n",
    "    '9',\n",
    "    'Dataset Completo Balanceado - Nao Normalizado',\n",
    "    max_alpha_balanced,\n",
    "    df_mean_balanced.loc[metrica]['Métrica'],\n",
    "    df_mean_balanced.loc[metrica]['Value'],\n",
    "    '[{:.3f}, {:.3f}, {:.3f}]'.format(mean_balanced[metrica],score_min_balanced.loc[metrica]['Score'], score_max_balanced.loc[metrica]['Score'])\n",
    "    ]\n",
    "    aux_df = (pd.DataFrame(result).T).rename(columns = {0:'Algoritmo' , 1: 'Atividade', 2: 'Condicao',\n",
    "                                                        3: 'Melhores_Params', 4:'Metrica', 5: 'Valor', 6: 'Intervalo_Confianca'})\n",
    "    all_models_results = all_models_results.append(aux_df, ignore_index=True)\n",
    "\n",
    "display(all_models_results)\n",
    "\n",
    "# salva os resultados novos\n",
    "try:\n",
    "    all_models_results.to_excel(path_to_save+'all_models_results.xlsx', index=False)\n",
    "    print('Resultados salvos com sucesso')\n",
    "except Exception as e:\n",
    "    print('Erro {}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
